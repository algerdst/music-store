import requests
from bs4 import BeautifulSoup
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
import glob
import os
import openpyxl


url=input('–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É')
headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 YaBrowser/24.4.0.0 Safari/537.36'
}


def make_description(x):
    description_text = f'<p>{x}  –ø–æ–¥ –∑–∞–∫–∞–∑ –∏–∑ –ï–≤—Ä–æ–ø—ã.</p> <p>–ú–∞–≥–∞–∑–∏–Ω—ã Musicstore, Thomann –∏ Promusictools.</p> <p>–î–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–Ω–∏–º–∞–µ—Ç –æ—Ç 2-3 –Ω–µ–¥–µ–ª–∏ —Å –º–æ–º–µ–Ω—Ç–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏, –ø–æ —Å—Ä–æ–∫–∞–º –Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ, –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–ª–∏—á–∏—è.</p> <p>üöõ O—Ç–øpa–≤–ª—é –ø–æ –≤—Åe–π Pocc–∏–∏.</p> <p>‚úÖ –¢–∞–∫ –∂–µ –º–æ–∂–µ—Ç–µ –≤—ã–±pa—Ç—å –ª—é–±—É—é –¥p—É–≥—É—é –≥–∏—Ç–∞p—É, —ç–ª–µ–∫—Ç—Ä–æ–≥–∏—Ç–∞—Ä—É, –∞–∫—É—Å—Ç–∏—á–µ—Å–∫—É—é –≥–∏—Ç–∞—Ä—É, —É—Å–∏–ª–∏—Ç–µ–ª–∏, –≥–æ–ª–æ–≤—ã, –∫–æ—Ñ—Ä –∏–ª–∏ –∫–µ–π—Å –ø–æ–¥ –∑–∞–∫–∞–∑, –¥–∞–∂–µ –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç –≤ –º–æ–∏—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è—Ö, –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ, —è —Ä–∞—Å—Å—á–∏—Ç–∞—é —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è —Å —É—á–µ—Ç–æ–º –¥–æ—Å—Ç–∞–≤–∫–∏.</p> <p>‚úî –ìA–†AHT–ò–Ø –ù–ê –¢O–í–ê–†</p> <p>üì® –ü–∏—à–∏—Ç–µ</p> <p>üìû –ó–≤–æ–Ω–∏—Ç–µ</p> <p>üë®‚Äçüîß–ë–µ–∑ –≤—ã—Ö–æ–¥–Ω—ã—Ö</p>'
    return description_text


def get_links():
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
    """
    links = []
    response = requests.get(url, headers=headers).text
    soup = BeautifulSoup(response, 'lxml')
    pages = int(soup.find('ul', 'paging-list').find_all('li')[-2].text)
    for page in range(pages):
        if page == 0:
            response = response
        else:
            if '?SortingAttribute' not in url:
                link = f"{url}/{page}"
            else:
                link = url.split('?SortingAttribute')
                link = f"{link[0]}/{page}?SortingAttribute{link[1]}"
            response = requests.get(link, headers=headers).text
        soup = BeautifulSoup(response, 'lxml')
        blocks = soup.find_all('div', class_='ident')
        for block in blocks:
            item_link = block.find('a', 'js-tracking')['href']
            links.append(item_link)
        time.sleep(1)
    return links

def get_info():
    file = []
    path = os.getcwd()
    for filename in glob.glob(os.path.join(path, '*.xlsx')):
        file.append(filename)
    filename = file[0]
    book = openpyxl.load_workbook(filename)
    sheet = book.active
    links=get_links()
    with webdriver.Chrome() as browser:
        row = 2
        count=0
        for item_link in links:
            # print(item_link)
            browser.get(item_link)
            time.sleep(1)
            title = browser.find_element(By.CSS_SELECTOR, 'h1').text
            description = make_description(title)
            features = browser.find_element(By.CSS_SELECTOR, 'div.feature-box').find_elements(By.TAG_NAME, 'li')
            for feature in features:
                description += '\n' + feature.text
            price = browser.find_element(By.CLASS_NAME, 'js-product-price-box').find_element(By.CSS_SELECTOR,
                                                                                             'span.kor-product-sale-price-value').text.strip(
                '‚Ç¨').strip()
            article=browser.find_element(By.CSS_SELECTOR, 'span.artnr').text.strip('–¢–æ–≤–∞—Ä: ')
            images=[]
            images_block=browser.find_element(By.CSS_SELECTOR, 'div.product-image').find_elements(By.CSS_SELECTOR, 'div.slick-slide')
            for image in images_block:
                try:
                    image=image.find_element(By.TAG_NAME,'img')
                    image_link=image.get_attribute('src')
                except:
                    break
                try:
                    image_link=image_link.replace('0160', '0640')
                except:
                    image_link=image.get_attribute('data-lazy')
                    image_link='https:'+image_link.replace('0160', '0640')
                if len(images)>=10:
                    break
                else:
                    images.append(image_link)
            images=' | '.join(images)
            yotube_request='https://www.youtube.com/results?search_query='+title.replace(' ','+').replace('&','%26')
            browser.get(yotube_request)
            time.sleep(1)
            video_youtube_link=browser.find_element(By.ID, 'video-title').get_attribute('href')
            sheet.cell(column=3, row=row).value = article
            sheet.cell(column=4, row=row).value = video_youtube_link
            sheet.cell(column=5, row=row).value = price
            sheet.cell(column=13, row=row).value = title
            sheet.cell(column=15, row=row).value = description
            sheet.cell(column=20, row=row).value = images
            row += 1
            book.save(filename)
            count+=1
            print(f'–û—Å—Ç–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Å—ã–ª–æ–∫ {len(links)-count}')

get_info()